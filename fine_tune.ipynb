{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYnz-fKW5yNT"
      },
      "outputs": [],
      "source": [
        "import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s63fi5q538Y"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE5QKbd785qk",
        "outputId": "727fa45a-9387-483c-dfb1-d51af19976a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-15 03:00:41--  https://huggingface.co/datasets/lehoangan02/nlp/resolve/main/UIT_HWDB_word_clean.zip?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 108.157.142.55, 108.157.142.53, 108.157.142.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.157.142.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/693ae93f73433849123ec646/d338f13a584c1bbba449389c040a56b5a9d970c5773bfb16c98d29a23138dbd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251215T030041Z&X-Amz-Expires=3600&X-Amz-Signature=4f1fb37f873d343dc9185ceaa51d16a6d4a142b8f9e18aab5f9901100cacc38f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27UIT_HWDB_word_clean.zip%3B+filename%3D%22UIT_HWDB_word_clean.zip%22%3B&response-content-type=application%2Fzip&x-id=GetObject&Expires=1765771241&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTc3MTI0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTNhZTkzZjczNDMzODQ5MTIzZWM2NDYvZDMzOGYxM2E1ODRjMWJiYmE0NDkzODljMDQwYTU2YjVhOWQ5NzBjNTc3M2JmYjE2Yzk4ZDI5YTIzMTM4ZGJkNioifV19&Signature=k6j2dZeL-PHTZfYwZybjZLnkGO1VEroZ419wqjygrhozEq0Uy-JSPkVk1UYyn0Cogo8uPdcTexMNNlhUff6MJrqLmiyOfmpkLqU07GssCfkFDNIda9LM4y2noHhmGsLll4ehPJB3ZvhVwzjIiCyiAFbFyptkhXngKdFD5YCMhevdMxgNxB2HdWnaKonn5FXKESrpMoCmTh%7ExCgIkKmM1TQkxNSrldtfavILBaIjz-s2L63jizRWbEAfxbxd5Xlu1mNECEsLGQ1LOrhlxz2YIeSeSFyFoNtJVHW73rsh%7EG24jcRQlDLjRffLyXAWcaC6ts4RmjrbebsmULjZ0-fnLPw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-15 03:00:41--  https://cas-bridge.xethub.hf.co/xet-bridge-us/693ae93f73433849123ec646/d338f13a584c1bbba449389c040a56b5a9d970c5773bfb16c98d29a23138dbd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251215T030041Z&X-Amz-Expires=3600&X-Amz-Signature=4f1fb37f873d343dc9185ceaa51d16a6d4a142b8f9e18aab5f9901100cacc38f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27UIT_HWDB_word_clean.zip%3B+filename%3D%22UIT_HWDB_word_clean.zip%22%3B&response-content-type=application%2Fzip&x-id=GetObject&Expires=1765771241&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTc3MTI0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTNhZTkzZjczNDMzODQ5MTIzZWM2NDYvZDMzOGYxM2E1ODRjMWJiYmE0NDkzODljMDQwYTU2YjVhOWQ5NzBjNTc3M2JmYjE2Yzk4ZDI5YTIzMTM4ZGJkNioifV19&Signature=k6j2dZeL-PHTZfYwZybjZLnkGO1VEroZ419wqjygrhozEq0Uy-JSPkVk1UYyn0Cogo8uPdcTexMNNlhUff6MJrqLmiyOfmpkLqU07GssCfkFDNIda9LM4y2noHhmGsLll4ehPJB3ZvhVwzjIiCyiAFbFyptkhXngKdFD5YCMhevdMxgNxB2HdWnaKonn5FXKESrpMoCmTh%7ExCgIkKmM1TQkxNSrldtfavILBaIjz-s2L63jizRWbEAfxbxd5Xlu1mNECEsLGQ1LOrhlxz2YIeSeSFyFoNtJVHW73rsh%7EG24jcRQlDLjRffLyXAWcaC6ts4RmjrbebsmULjZ0-fnLPw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 54.192.100.7, 54.192.100.74, 54.192.100.121, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|54.192.100.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 54085048 (52M) [application/zip]\n",
            "Saving to: ‘UIT_HWDB_word_clean.zip?download=true’\n",
            "\n",
            "UIT_HWDB_word_clean 100%[===================>]  51.58M  69.1MB/s    in 0.7s    \n",
            "\n",
            "2025-12-15 03:00:42 (69.1 MB/s) - ‘UIT_HWDB_word_clean.zip?download=true’ saved [54085048/54085048]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/lehoangan02/nlp/resolve/main/UIT_HWDB_word_clean.zip?download=true\n",
        "!unzip UIT_HWDB_word_clean.zip?download=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxysi2cY84OY"
      },
      "source": [
        "LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzOR8Ns9DpF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "BASE = \"UIT_HWDB_word_clean/train\"\n",
        "\n",
        "with open(f\"{BASE}/labels.json\", encoding=\"utf-8\") as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "samples = []\n",
        "for img_name, text in labels.items():\n",
        "    samples.append({\n",
        "        \"image_path\": f\"{BASE}/images/{img_name}\",\n",
        "        \"text\": text.strip()\n",
        "    })\n",
        "\n",
        "raw_train_ds = Dataset.from_list(samples)\n",
        "raw_train_ds[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xacYS-Js83qG"
      },
      "outputs": [],
      "source": [
        "instruction = \"<image>\\nRecognize the word.\"\n",
        "\n",
        "def to_messages(example):\n",
        "    img = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": instruction}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": example[\"text\"]}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "train_ds = raw_train_ds.map(\n",
        "    to_messages,\n",
        "    remove_columns=raw_train_ds.column_names\n",
        ")\n",
        "\n",
        "train_ds[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRqwa9246AiX"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/DeepSeek-OCR\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhogyMEX9f0J"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ocr_lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXRnMMn46Ka4"
      },
      "outputs": [],
      "source": [
        "# @title Create datacollator\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from PIL import Image, ImageOps\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import io\n",
        "\n",
        "from deepseek_ocr.modeling_deepseekocr import (\n",
        "    format_messages,\n",
        "    text_encode,\n",
        "    BasicImageTransform,\n",
        "    dynamic_preprocess,\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class DeepSeekOCRDataCollator:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tokenizer: Tokenizer\n",
        "        model: Model\n",
        "        image_size: Size for image patches (default: 640)\n",
        "        base_size: Size for global view (default: 1024)\n",
        "        crop_mode: Whether to use dynamic cropping for large images\n",
        "        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n",
        "    \"\"\"\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    image_size: int = 640\n",
        "    base_size: int = 1024\n",
        "    crop_mode: bool = True\n",
        "    image_token_id: int = 128815\n",
        "    train_on_responses_only: bool = True\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_size: int = 640,\n",
        "        base_size: int = 1024,\n",
        "        crop_mode: bool = True,\n",
        "        train_on_responses_only: bool = True,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.crop_mode = crop_mode\n",
        "        self.image_token_id = 128815\n",
        "        self.dtype = model.dtype  # Get dtype from model\n",
        "        self.train_on_responses_only = train_on_responses_only\n",
        "\n",
        "        self.image_transform = BasicImageTransform(\n",
        "            mean=(0.5, 0.5, 0.5),\n",
        "            std=(0.5, 0.5, 0.5),\n",
        "            normalize=True\n",
        "        )\n",
        "        self.patch_size = 16\n",
        "        self.downsample_ratio = 4\n",
        "\n",
        "        # Get BOS token ID from tokenizer\n",
        "        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n",
        "            self.bos_id = tokenizer.bos_token_id\n",
        "        else:\n",
        "            self.bos_id = 0\n",
        "            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n",
        "\n",
        "    def deserialize_image(self, image_data) -> Image.Image:\n",
        "        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n",
        "        if isinstance(image_data, Image.Image):\n",
        "            return image_data.convert(\"RGB\")\n",
        "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "            image_bytes = image_data['bytes']\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            return image.convert(\"RGB\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
        "\n",
        "    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n",
        "        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n",
        "        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "        width_crop_num, height_crop_num = crop_ratio\n",
        "\n",
        "        if self.crop_mode:\n",
        "            img_tokens = num_queries_base * num_queries_base + 1\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                img_tokens += (num_queries * width_crop_num + 1) * (num_queries * height_crop_num)\n",
        "        else:\n",
        "            img_tokens = num_queries * num_queries + 1\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Process a single image based on crop_mode and size thresholds\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n",
        "        \"\"\"\n",
        "        images_list = []\n",
        "        images_crop_list = []\n",
        "        images_spatial_crop = []\n",
        "\n",
        "        if self.crop_mode:\n",
        "            # Determine crop ratio based on image size\n",
        "            if image.size[0] <= 640 and image.size[1] <= 640:\n",
        "                crop_ratio = (1, 1)\n",
        "                images_crop_raw = []\n",
        "            else:\n",
        "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
        "                    image, min_num=2, max_num=9,\n",
        "                    image_size=self.image_size, use_thumbnail=False\n",
        "                )\n",
        "\n",
        "            # Process global view with padding\n",
        "            global_view = ImageOps.pad(\n",
        "                image, (self.base_size, self.base_size),\n",
        "                color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "            )\n",
        "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            width_crop_num, height_crop_num = crop_ratio\n",
        "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "\n",
        "            # Process local views (crops) if applicable\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                for crop_img in images_crop_raw:\n",
        "                    images_crop_list.append(\n",
        "                        self.image_transform(crop_img).to(self.dtype)\n",
        "                    )\n",
        "\n",
        "            # Calculate image tokens\n",
        "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                    num_queries * height_crop_num)\n",
        "\n",
        "        else:  # crop_mode = False\n",
        "            crop_ratio = (1, 1)\n",
        "            images_spatial_crop.append([1, 1])\n",
        "\n",
        "            # For smaller base sizes, resize; for larger, pad\n",
        "            if self.base_size <= 640:\n",
        "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
        "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
        "            else:\n",
        "                global_view = ImageOps.pad(\n",
        "                    image, (self.base_size, self.base_size),\n",
        "                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                )\n",
        "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
        "\n",
        "    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Process a single conversation into model inputs.\n",
        "            \"\"\"\n",
        "\n",
        "            # --- 1. Setup ---\n",
        "            images = []\n",
        "            for message in messages:\n",
        "                for item in message[\"content\"]:\n",
        "                    if item[\"type\"] == \"image\":\n",
        "                        images.append(item[\"image\"].convert(\"RGB\"))\n",
        "\n",
        "            if not images:\n",
        "                raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n",
        "\n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list, images_crop_list, images_spatial_crop = [], [], []\n",
        "\n",
        "            prompt_token_count = -1 # Index to start training\n",
        "            assistant_started = False\n",
        "            image_idx = 0\n",
        "\n",
        "            # Add BOS token at the very beginning\n",
        "            tokenized_str.append(self.bos_id)\n",
        "            images_seq_mask.append(False)\n",
        "\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "\n",
        "                # Check if this is the assistant's turn\n",
        "                if role == \"assistant\":\n",
        "                    if not assistant_started:\n",
        "                        # This is the split point. All tokens added *so far*\n",
        "                        # are part of the prompt.\n",
        "                        prompt_token_count = len(tokenized_str)\n",
        "                        assistant_started = True\n",
        "\n",
        "                    # Append the EOS token string to the *end* of assistant content\n",
        "                    content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Split this message's content by the image token\n",
        "                text_splits = content.split('<image>')\n",
        "\n",
        "                for i, text_sep in enumerate(text_splits):\n",
        "                    # Tokenize the text part\n",
        "                    tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
        "                    tokenized_str.extend(tokenized_sep)\n",
        "                    images_seq_mask.extend([False] * len(tokenized_sep))\n",
        "\n",
        "                    # If this text is followed by an <image> tag\n",
        "                    if i < len(text_splits) - 1:\n",
        "                        if image_idx >= len(images):\n",
        "                            raise ValueError(\n",
        "                                f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n",
        "                            )\n",
        "\n",
        "                        # Process the image\n",
        "                        image = images[image_idx]\n",
        "                        img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n",
        "\n",
        "                        images_list.extend(img_list)\n",
        "                        images_crop_list.extend(crop_list)\n",
        "                        images_spatial_crop.extend(spatial_crop)\n",
        "\n",
        "                        # Add image placeholder tokens\n",
        "                        tokenized_str.extend(tok_img)\n",
        "                        images_seq_mask.extend([True] * len(tok_img))\n",
        "\n",
        "                        image_idx += 1 # Move to the next image\n",
        "\n",
        "            # --- 3. Validation and Final Prep ---\n",
        "            if image_idx != len(images):\n",
        "                raise ValueError(\n",
        "                    f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n",
        "                )\n",
        "\n",
        "            # If we never found an assistant message, we're in a weird state\n",
        "            # (e.g., user-only prompt). We mask everything.\n",
        "            if not assistant_started:\n",
        "                print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n",
        "                prompt_token_count = len(tokenized_str)\n",
        "\n",
        "            # Prepare image tensors\n",
        "            images_ori = torch.stack(images_list, dim=0)\n",
        "            images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n",
        "\n",
        "            if images_crop_list:\n",
        "                images_crop = torch.stack(images_crop_list, dim=0)\n",
        "            else:\n",
        "                images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n",
        "                \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n",
        "                \"images_ori\": images_ori,\n",
        "                \"images_crop\": images_crop,\n",
        "                \"images_spatial_crop\": images_spatial_crop_tensor,\n",
        "                \"prompt_token_count\": prompt_token_count, # This is now accurate\n",
        "            }\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of samples\"\"\"\n",
        "        batch_data = []\n",
        "\n",
        "        # Process each sample\n",
        "        for feature in features:\n",
        "            try:\n",
        "                processed = self.process_single_sample(feature['messages'])\n",
        "                batch_data.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_data:\n",
        "            raise ValueError(\"No valid samples in batch\")\n",
        "\n",
        "        # Extract lists\n",
        "        input_ids_list = [item['input_ids'] for item in batch_data]\n",
        "        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n",
        "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens (model shouldn't predict these)\n",
        "        labels[images_seq_mask] = -100\n",
        "\n",
        "        # Mask user prompt tokens when train_on_responses_only=True (only train on assistant responses)\n",
        "        if self.train_on_responses_only:\n",
        "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
        "                if prompt_count > 0:\n",
        "                    labels[idx, :prompt_count] = -100\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        # Prepare images batch (list of tuples)\n",
        "        images_batch = []\n",
        "        for item in batch_data:\n",
        "            images_batch.append((item['images_crop'], item['images_ori']))\n",
        "\n",
        "        # Stack spatial crop info\n",
        "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask,\n",
        "            \"images_spatial_crop\": images_spatial_crop,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DeepSeekOCRDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    train_on_responses_only=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfWoSH2k6LI_"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbAr4euq6S77"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_nWol7j6W3z"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
